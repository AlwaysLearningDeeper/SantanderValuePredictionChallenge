{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FastICA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gc\n",
    "#Plotting\n",
    "import seaborn as sns\n",
    "\n",
    "#Models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the files and get a brief overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../raw_files/train.csv')\n",
    "test_df = pd.read_csv('../raw_files/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>48df886f9</th>\n",
       "      <th>0deb4b6a8</th>\n",
       "      <th>34b15f335</th>\n",
       "      <th>a8cb14b00</th>\n",
       "      <th>2f0771a37</th>\n",
       "      <th>30347e683</th>\n",
       "      <th>d08d1fbe3</th>\n",
       "      <th>6ee66e115</th>\n",
       "      <th>...</th>\n",
       "      <th>3ecc09859</th>\n",
       "      <th>9281abeea</th>\n",
       "      <th>8675bec0b</th>\n",
       "      <th>3a13ed79a</th>\n",
       "      <th>f677d4d13</th>\n",
       "      <th>71b203550</th>\n",
       "      <th>137efaa80</th>\n",
       "      <th>fb36b89d9</th>\n",
       "      <th>7e293fbaf</th>\n",
       "      <th>9fc776466</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d6aaf2</td>\n",
       "      <td>38000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fbd867</td>\n",
       "      <td>600000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0027d6b71</td>\n",
       "      <td>10000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0028cbf45</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002a68644</td>\n",
       "      <td>14400000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4993 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID      target  48df886f9  0deb4b6a8  34b15f335  a8cb14b00  \\\n",
       "0  000d6aaf2  38000000.0        0.0          0        0.0          0   \n",
       "1  000fbd867    600000.0        0.0          0        0.0          0   \n",
       "2  0027d6b71  10000000.0        0.0          0        0.0          0   \n",
       "3  0028cbf45   2000000.0        0.0          0        0.0          0   \n",
       "4  002a68644  14400000.0        0.0          0        0.0          0   \n",
       "\n",
       "   2f0771a37  30347e683  d08d1fbe3  6ee66e115    ...      3ecc09859  \\\n",
       "0          0          0          0          0    ...            0.0   \n",
       "1          0          0          0          0    ...            0.0   \n",
       "2          0          0          0          0    ...            0.0   \n",
       "3          0          0          0          0    ...            0.0   \n",
       "4          0          0          0          0    ...            0.0   \n",
       "\n",
       "   9281abeea  8675bec0b  3a13ed79a  f677d4d13  71b203550  137efaa80  \\\n",
       "0        0.0        0.0          0          0          0          0   \n",
       "1        0.0        0.0          0          0          0          0   \n",
       "2        0.0        0.0          0          0          0          0   \n",
       "3        0.0        0.0          0          0          0          0   \n",
       "4        0.0        0.0          0          0          0          0   \n",
       "\n",
       "   fb36b89d9  7e293fbaf  9fc776466  \n",
       "0          0          0          0  \n",
       "1          0          0          0  \n",
       "2          0          0          0  \n",
       "3          0          0          0  \n",
       "4          0          0          0  \n",
       "\n",
       "[5 rows x 4993 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>48df886f9</th>\n",
       "      <th>0deb4b6a8</th>\n",
       "      <th>34b15f335</th>\n",
       "      <th>a8cb14b00</th>\n",
       "      <th>2f0771a37</th>\n",
       "      <th>30347e683</th>\n",
       "      <th>d08d1fbe3</th>\n",
       "      <th>6ee66e115</th>\n",
       "      <th>20aa07010</th>\n",
       "      <th>...</th>\n",
       "      <th>3ecc09859</th>\n",
       "      <th>9281abeea</th>\n",
       "      <th>8675bec0b</th>\n",
       "      <th>3a13ed79a</th>\n",
       "      <th>f677d4d13</th>\n",
       "      <th>71b203550</th>\n",
       "      <th>137efaa80</th>\n",
       "      <th>fb36b89d9</th>\n",
       "      <th>7e293fbaf</th>\n",
       "      <th>9fc776466</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000137c73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00021489f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004d7953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00056a333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00056d8eb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4992 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  48df886f9  0deb4b6a8  34b15f335  a8cb14b00  2f0771a37  \\\n",
       "0  000137c73        0.0        0.0        0.0        0.0        0.0   \n",
       "1  00021489f        0.0        0.0        0.0        0.0        0.0   \n",
       "2  0004d7953        0.0        0.0        0.0        0.0        0.0   \n",
       "3  00056a333        0.0        0.0        0.0        0.0        0.0   \n",
       "4  00056d8eb        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "   30347e683  d08d1fbe3  6ee66e115  20aa07010    ...      3ecc09859  \\\n",
       "0        0.0        0.0        0.0        0.0    ...            0.0   \n",
       "1        0.0        0.0        0.0        0.0    ...            0.0   \n",
       "2        0.0        0.0        0.0        0.0    ...            0.0   \n",
       "3        0.0        0.0        0.0        0.0    ...            0.0   \n",
       "4        0.0        0.0        0.0        0.0    ...            0.0   \n",
       "\n",
       "   9281abeea  8675bec0b  3a13ed79a  f677d4d13  71b203550  137efaa80  \\\n",
       "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "1        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "2        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "   fb36b89d9  7e293fbaf  9fc776466  \n",
       "0        0.0        0.0        0.0  \n",
       "1        0.0        0.0        0.0  \n",
       "2        0.0        0.0        0.0  \n",
       "3        0.0        0.0        0.0  \n",
       "4        0.0        0.0        0.0  \n",
       "\n",
       "[5 rows x 4992 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up train and test X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop([\"ID\", \"target\"], axis=1)\n",
    "y_train = np.log1p(train_df[\"target\"].values)\n",
    "\n",
    "X_test = test_df.drop([\"ID\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4459 entries, 0 to 4458\n",
      "Columns: 4993 entries, ID to 9fc776466\n",
      "dtypes: float64(1845), int64(3147), object(1)\n",
      "memory usage: 169.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49342 entries, 0 to 49341\n",
      "Columns: 4992 entries, ID to 9fc776466\n",
      "dtypes: float64(4991), object(1)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for NaN values and removing constant features in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Train Features with NaN Values = 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total Train Features with NaN Values = \" + str(train_df.columns[train_df.isnull().sum() != 0].size))\n",
    "if (train_df.columns[train_df.isnull().sum() != 0].size):\n",
    "    print(\"Features with NaN => {}\".format(list(train_df.columns[train_df.isnull().sum() != 0])))\n",
    "    train_df[train_df.columns[train_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2362 features of 4491 have zeroes in 99% or more samples.\n",
      "2868 features of 4491 have zeroes in 98% or more samples.\n",
      "3315 features of 4491 have zeroes in 97% or more samples.\n",
      "3794 features of 4491 have zeroes in 96% or more samples.\n",
      "3992 features of 4491 have zeroes in 95% or more samples.\n",
      "\n",
      "Train shape: (4459, 2123)\n",
      "Test shape: (49342, 2123)\n"
     ]
    }
   ],
   "source": [
    "zero_count = []\n",
    "for col in X_train.columns[2:]:\n",
    "    zero_count.append([i[1] for i in list(X_train[col].value_counts().items()) if i[0] == 0][0])\n",
    "    \n",
    "print('{0} features of 4491 have zeroes in 99% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.99])))\n",
    "print('{0} features of 4491 have zeroes in 98% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.98])))\n",
    "print('{0} features of 4491 have zeroes in 97% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.97])))\n",
    "print('{0} features of 4491 have zeroes in 96% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.96])))\n",
    "print('{0} features of 4491 have zeroes in 95% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.95])))\n",
    "\n",
    "cols_to_drop = [col for col in X_train.columns[2:] if [i[1] for i in list(X_train[col].value_counts().items()) if i[0] == 0][0] >= 4459 * 0.98]\n",
    "\n",
    "X_train.drop(cols_to_drop, axis=1, inplace=True)\n",
    "X_test.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colsToRemove = []\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].std() == 0: \n",
    "        colsToRemove.append(col)\n",
    "        \n",
    "# remove constant columns in the training set\n",
    "train_df.drop(colsToRemove, axis=1, inplace=True)\n",
    "\n",
    "# remove constant columns in the test set\n",
    "test_df.drop(colsToRemove, axis=1, inplace=True) \n",
    "\n",
    "print(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\n",
    "print(colsToRemove)\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToRemove = []\n",
    "colsScaned = []\n",
    "dupList = {}\n",
    "\n",
    "columns = X_train.columns\n",
    "\n",
    "for i in range(len(columns)-1):\n",
    "    v = X_train[columns[i]].values\n",
    "    dupCols = []\n",
    "    for j in range(i+1,len(columns)):\n",
    "        if np.array_equal(v, X_train[columns[j]].values):\n",
    "            colsToRemove.append(columns[j])\n",
    "            if columns[j] not in colsScaned:\n",
    "                dupCols.append(columns[j]) \n",
    "                colsScaned.append(columns[j])\n",
    "                dupList[columns[i]] = dupCols\n",
    "                \n",
    "# remove duplicate columns in the training set\n",
    "X_train.drop(colsToRemove, axis=1, inplace=True) \n",
    "\n",
    "# remove duplicate columns in the testing set\n",
    "X_test.drop(colsToRemove, axis=1, inplace=True)\n",
    "\n",
    "print(\"Removed `{}` Duplicate Columns\\n\".format(len(dupList)))\n",
    "print(dupList)\n",
    "\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Sparse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_sparse(train, test):\n",
    "    flist = [x for x in train.columns if not x in ['ID','target']]\n",
    "    for f in flist:\n",
    "        if len(np.unique(train[f]))<2:\n",
    "            train.drop(f, axis=1, inplace=True)\n",
    "            test.drop(f, axis=1, inplace=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drop_sparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c157a791d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTrain shape: {}\\nTest shape: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drop_sparse' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test = drop_sparse(X_train, X_test)\n",
    "\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumzeros and Sumvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_SumZeros(train, test, features):\n",
    "    flist = [x for x in train.columns if not x in ['ID','target']]\n",
    "    if 'SumZeros' in features:\n",
    "        train.insert(1, 'SumZeros', (train[flist] == 0).astype(int).sum(axis=1))\n",
    "        test.insert(1, 'SumZeros', (test[flist] == 0).astype(int).sum(axis=1))\n",
    "    flist = [x for x in train.columns if not x in ['ID','target']]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test = add_SumZeros(X_train, X_test, ['SumZeros'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_SumValues(train, test, features):\n",
    "    flist = [x for x in train.columns if not x in ['ID','target']]\n",
    "    if 'SumValues' in features:\n",
    "        train.insert(1, 'SumValues', (train[flist] != 0).astype(int).sum(axis=1))\n",
    "        test.insert(1, 'SumValues', (test[flist] != 0).astype(int).sum(axis=1))\n",
    "    flist = [x for x in train.columns if not x in ['ID','target']]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test = add_SumValues(X_train, X_test, ['SumValues'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_OtherAgg(train, test, features):\n",
    "    flist = [x for x in train.columns if not x in ['ID','target','SumZeros','SumValues']]\n",
    "    if 'OtherAgg' in features:\n",
    "        train['Mean'] = train.mean(axis=1)\n",
    "        train['Median'] = train.median(axis=1)\n",
    "        train['Mode'] = train.mode(axis=1)\n",
    "        train['Max'] = train.max(axis=1)\n",
    "        train['Var'] = train.var(axis=1)\n",
    "        train['Std'] = train.std(axis=1)\n",
    "        \n",
    "        test['Mean'] = test.mean(axis=1)\n",
    "        test['Median'] = test.median(axis=1)\n",
    "        test['Mode'] = test.mode(axis=1)\n",
    "        test['Max'] = test.max(axis=1)\n",
    "        test['Var'] = test.var(axis=1)\n",
    "        test['Std'] = test.std(axis=1)\n",
    "    flist = [x for x in train.columns if not x in ['ID','target','SumZeros','SumValues']]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kmeans_cluster_2', 'kmeans_cluster_3', 'kmeans_cluster_4', 'kmeans_cluster_5', 'kmeans_cluster_6', 'kmeans_cluster_7', 'kmeans_cluster_8', 'kmeans_cluster_9', 'kmeans_cluster_10']\n"
     ]
    }
   ],
   "source": [
    "def kmeans(X_Tr,Xte):\n",
    "    flist = [x for x in X_Tr.columns if not x in ['ID','target']]\n",
    "    flist_kmeans = []\n",
    "    for ncl in range(2,11):\n",
    "        cls = KMeans(n_clusters=ncl)\n",
    "        cls.fit_predict(X_train[flist].values)\n",
    "        X_Tr['kmeans_cluster_'+str(ncl)] = cls.predict(X_Tr[flist].values)\n",
    "        Xte['kmeans_cluster_'+str(ncl)] = cls.predict(Xte[flist].values)\n",
    "        flist_kmeans.append('kmeans_cluster_'+str(ncl))\n",
    "    print(flist_kmeans)\n",
    "    \n",
    "    return X_Tr,Xte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PCA_1', 'PCA_2', 'PCA_3', 'PCA_4', 'PCA_5', 'PCA_6', 'PCA_7', 'PCA_8', 'PCA_9', 'PCA_10', 'PCA_11', 'PCA_12', 'PCA_13', 'PCA_14', 'PCA_15', 'PCA_16', 'PCA_17', 'PCA_18', 'PCA_19', 'PCA_20']\n"
     ]
    }
   ],
   "source": [
    "def pca(X_Tr,Xte):\n",
    "    flist = [x for x in X_Tr.columns if not x in ['ID','target']]\n",
    "    n_components = 20\n",
    "    flist_pca = []\n",
    "    pca = PCA(n_components=n_components)\n",
    "    x_train_projected = pca.fit_transform(normalize(X_Tr[flist], axis=0))\n",
    "    x_test_projected = pca.transform(normalize(X_test[flist], axis=0))\n",
    "    for npca in range(0, n_components):\n",
    "        X_Tr.insert(1, 'PCA_'+str(npca+1), x_train_projected[:, npca])\n",
    "        Xte.insert(1, 'PCA_'+str(npca+1), x_test_projected[:, npca])\n",
    "        flist_pca.append('PCA_'+str(npca+1))\n",
    "    print(flist_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (4459, 2123)\n",
      "Test shape: (49342, 2123)\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start decomposition process...\n",
      "PCA\n",
      "tSVD\n",
      "ICA\n",
      "GRP\n",
      "SRP\n",
      "Append decomposition components to datasets...\n",
      "\n",
      "Train shape: (4459, 2223)\n",
      "Test shape: (49342, 2223)\n"
     ]
    }
   ],
   "source": [
    "PERC_TRESHOLD = 0.98   ### Percentage of zeros in each feature ###\n",
    "N_COMP = 20            ### Number of decomposition components ###\n",
    "\n",
    "print(\"\\nStart decomposition process...\")\n",
    "print(\"PCA\")\n",
    "pca = PCA(n_components=N_COMP, random_state=17)\n",
    "pca_results_train = pca.fit_transform(X_train)\n",
    "pca_results_test = pca.transform(X_test)\n",
    "\n",
    "print(\"tSVD\")\n",
    "tsvd = TruncatedSVD(n_components=N_COMP, random_state=17)\n",
    "tsvd_results_train = tsvd.fit_transform(X_train)\n",
    "tsvd_results_test = tsvd.transform(X_test)\n",
    "\n",
    "print(\"ICA\")\n",
    "ica = FastICA(n_components=N_COMP, random_state=17)\n",
    "ica_results_train = ica.fit_transform(X_train)\n",
    "ica_results_test = ica.transform(X_test)\n",
    "\n",
    "print(\"GRP\")\n",
    "grp = GaussianRandomProjection(n_components=N_COMP, eps=0.1, random_state=17)\n",
    "grp_results_train = grp.fit_transform(X_train)\n",
    "grp_results_test = grp.transform(X_test)\n",
    "\n",
    "print(\"SRP\")\n",
    "srp = SparseRandomProjection(n_components=N_COMP, dense_output=True, random_state=17)\n",
    "srp_results_train = srp.fit_transform(X_train)\n",
    "srp_results_test = srp.transform(X_test)\n",
    "\n",
    "print(\"Append decomposition components to datasets...\")\n",
    "for i in range(1, N_COMP + 1):\n",
    "    X_train['pca_' + str(i)] = pca_results_train[:, i - 1]\n",
    "    X_test['pca_' + str(i)] = pca_results_test[:, i - 1]\n",
    "\n",
    "    X_train['ica_' + str(i)] = ica_results_train[:, i - 1]\n",
    "    X_test['ica_' + str(i)] = ica_results_test[:, i - 1]\n",
    "\n",
    "    X_train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    X_test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    X_train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    X_test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    X_train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    X_test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params =  {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_leaves\": 100,\n",
    "    \"feature_fraction\": .5,\n",
    "    \"bagging_fraction\": .5,\n",
    "    #'bagging_freq': 4,\n",
    "    \"max_depth\": -1,\n",
    "    \"reg_alpha\": 0.3,\n",
    "    \"reg_lambda\": 0.1,\n",
    "    \"min_child_weight\":10,\n",
    "    'zero_as_missing':True\n",
    "}\n",
    "bestparams = {0: (387, 0.32549223081879874),\n",
    "             1: (363, 0.48964347828583543),\n",
    "             2: (389, 0.5646998803516646),\n",
    "             3: (412, 0.5738770523457457),\n",
    "             4: (371, 0.5016286734255448),\n",
    "             5: (433, 0.3353076747094824)}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros((X_train.shape[0],6))\n",
    "sub_preds = np.zeros((X_test.shape[0],6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bestparams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b3834a2f2e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0moptimal_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_cv_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbestparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_cv_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bestparams' is not defined"
     ]
    }
   ],
   "source": [
    "for i in [0, 1, 2, 3, 4, 5]:\n",
    "    optimal_rounds, best_cv_score = bestparams[i]\n",
    "    print(i, optimal_rounds, best_cv_score)\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train)):\n",
    "        print(n_fold)\n",
    "        trn_x, trn_y = X_train[feats].iloc[trn_idx], x[trn_idx]>i\n",
    "        val_x, val_y = X_train[feats].iloc[val_idx], x[val_idx]>i\n",
    "        \n",
    "        clf = lgbm.train(lgbm_params,\n",
    "                         lgbm.Dataset(trn_x,trn_y),\n",
    "                         num_boost_round = optimal_rounds + 1,\n",
    "                         verbose_eval=200)\n",
    "\n",
    "        oof_preds[val_idx,i] = clf.predict(val_x, num_iteration=optimal_rounds + 1)\n",
    "        sub_preds[:,i] += clf.predict(test[feats], num_iteration=optimal_rounds + 1) / folds.n_splits\n",
    "\n",
    "        del clf\n",
    "        del trn_x, trn_y, val_x, val_y\n",
    "        gc.collect()\n",
    "\n",
    "off_preds_withbias = np.hstack([oof_preds,np.ones(shape=(oof_preds.shape[0],1))])\n",
    "sub_preds_withbias = np.hstack([sub_preds,np.ones(shape=(sub_preds.shape[0],1))])\n",
    "\n",
    "params = np.linalg.lstsq(off_preds_withbias, train.target)[0]\n",
    "\n",
    "trainpreds = np.dot(off_preds_withbias,params)\n",
    "print(np.sqrt(mean_squared_error(train.target,trainpreds)))\n",
    "\n",
    "testpreds = np.dot(sub_preds_withbias,params)\n",
    "subLGBM = pd.DataFrame({'ID':test.ID, 'target':np.expm1(testpreds)})\n",
    "\n",
    "lg_an = np.expm1(testpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"num_leaves\" : 40,\n",
    "        \"learning_rate\" : 0.005,\n",
    "        \"bagging_fraction\" : 0.7,\n",
    "        \"feature_fraction\" : 0.5,\n",
    "        \"bagging_frequency\" : 5,\n",
    "        \"bagging_seed\" : 42,\n",
    "        \"verbosity\" : -1,\n",
    "        \"random_seed\": 42\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgtrain, 5000, \n",
    "                      valid_sets=[lgval], \n",
    "                      early_stopping_rounds=100, \n",
    "                      verbose_eval=50, \n",
    "                      evals_result=evals_result)\n",
    "    \n",
    "    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n",
    "    return pred_test_y, model, evals_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {'objective': 'reg:linear', \n",
    "          'eval_metric': 'rmse',\n",
    "          'eta': 0.005,\n",
    "          'max_depth': 15, \n",
    "          'subsample': 0.7, \n",
    "          'colsample_bytree': 0.5,\n",
    "          'alpha':0,\n",
    "          'random_state':42, \n",
    "          'silent': True}\n",
    "    \n",
    "    tr_data = xgb.DMatrix(train_X, train_y)\n",
    "    va_data = xgb.DMatrix(val_X, val_y)\n",
    "    \n",
    "    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n",
    "    \n",
    "    model_xgb = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds = 30, verbose_eval=100)\n",
    "    \n",
    "    dtest = xgb.DMatrix(test_X)\n",
    "    xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n",
    "    \n",
    "    return xgb_pred_y, model_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print \"\\nModel Report\"\n",
    "    print \"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions)\n",
    "    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob)\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CatBoost + decomposition features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FastICA\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from sklearn.model_selection import KFold\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "print(\"Load data...\")\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "subm = pd.read_csv('../input/sample_submission.csv')\n",
    "print(\"Train shape: {}\\nTest shape: {}\".format(train.shape, test.shape))\n",
    "col = [c for c in train.columns if c not in ['ID', 'target']]\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "def rmsle(y, pred):\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(pred), 2)))\n",
    "\n",
    "x1, x2, y1, y2 = model_selection.train_test_split(train[col], train.target.values, test_size=0.10, random_state=5)\n",
    "model = ensemble.RandomForestRegressor(n_jobs = -1, random_state = 7)\n",
    "model.fit(scl.fit_transform(x1), y1)\n",
    "print(rmsle(y2, model.predict(scl.transform(x2))))\n",
    "\n",
    "col = pd.DataFrame({'importance': model.feature_importances_, 'feature': col}).sort_values(by=['importance'], ascending=[False])[:600]['feature'].values\n",
    "\n",
    "#Added Columns from feature_selection\n",
    "train = train[['ID', 'target']+list(col)]\n",
    "test = test[['ID']+list(col)]\n",
    "print(\"Train shape: {}\\nTest shape: {}\".format(train.shape, test.shape))\n",
    "\n",
    "PERC_TRESHOLD = 0.98   ### Percentage of zeros in each feature ###\n",
    "N_COMP = 20            ### Number of decomposition components ###\n",
    "\n",
    "target = np.log1p(train['target']).values\n",
    "cols_to_drop = [col for col in train.columns[2:]\n",
    "                    if [i[1] for i in list(train[col].value_counts().items()) \n",
    "                    if i[0] == 0][0] >= train.shape[0] * PERC_TRESHOLD]\n",
    "\n",
    "print(\"Define training features...\")\n",
    "exclude_other = ['ID', 'target']\n",
    "train_features = []\n",
    "for c in train.columns:\n",
    "    if c not in cols_to_drop \\\n",
    "    and c not in exclude_other:\n",
    "        train_features.append(c)\n",
    "print(\"Number of featuress for training: %s\" % len(train_features))\n",
    "\n",
    "train, test = train[train_features], test[train_features]\n",
    "print(\"\\nTrain shape: {}\\nTest shape: {}\".format(train.shape, test.shape))\n",
    "\n",
    "print(\"\\nStart decomposition process...\")\n",
    "print(\"PCA\")\n",
    "pca = PCA(n_components=N_COMP, random_state=17)\n",
    "pca_results_train = pca.fit_transform(train)\n",
    "pca_results_test = pca.transform(test)\n",
    "\n",
    "print(\"tSVD\")\n",
    "tsvd = TruncatedSVD(n_components=N_COMP, random_state=17)\n",
    "tsvd_results_train = tsvd.fit_transform(train)\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "print(\"ICA\")\n",
    "ica = FastICA(n_components=N_COMP, random_state=17)\n",
    "ica_results_train = ica.fit_transform(train)\n",
    "ica_results_test = ica.transform(test)\n",
    "\n",
    "print(\"GRP\")\n",
    "grp = GaussianRandomProjection(n_components=N_COMP, eps=0.1, random_state=17)\n",
    "grp_results_train = grp.fit_transform(train)\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "print(\"SRP\")\n",
    "srp = SparseRandomProjection(n_components=N_COMP, dense_output=True, random_state=17)\n",
    "srp_results_train = srp.fit_transform(train)\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "print(\"Append decomposition components to datasets...\")\n",
    "for i in range(1, N_COMP + 1):\n",
    "    train['pca_' + str(i)] = pca_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(train.shape, test.shape))\n",
    "\n",
    "print('\\nModelling...')\n",
    "def rmsle(y_true, y_pred):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    return np.sqrt(np.mean(np.power(np.log(y_true + 1) - np.log(y_pred + 1), 2)))\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=546789)\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test.shape[0])\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train)):\n",
    "    trn_x, trn_y = train.ix[trn_idx], target[trn_idx]\n",
    "    val_x, val_y = train.ix[val_idx], target[val_idx]\n",
    "    cb_model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=4, l2_leaf_reg=20, bootstrap_type='Bernoulli', subsample=0.6, eval_metric='RMSE', metric_period=50, od_type='Iter', od_wait=45, random_seed=17, allow_writing_files=False)\n",
    "    cb_model.fit(trn_x, trn_y, eval_set=(val_x, val_y), cat_features=[], use_best_model=True, verbose=True)\n",
    "    oof_preds[val_idx] = cb_model.predict(val_x)\n",
    "    sub_preds += cb_model.predict(test) / folds.n_splits\n",
    "    print(\"Fold %2d RMSLE : %.6f\" % (n_fold+1, rmsle(np.exp(val_y)-1, np.exp(oof_preds[val_idx])-1)))\n",
    "\n",
    "print(\"Full RMSLE score %.6f\" % rmsle(np.exp(target)-1, np.exp(oof_preds)-1)) \n",
    "subm['target'] = np.exp(sub_preds)-1\n",
    "cb_ans = np.exp(sub_preds)\n",
    "subm.to_csv('CB_PCA_and_stuff.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmse: 1.60281\n",
      "[100]\tvalid_0's rmse: 1.53618\n",
      "[150]\tvalid_0's rmse: 1.48962\n",
      "[200]\tvalid_0's rmse: 1.4568\n",
      "[250]\tvalid_0's rmse: 1.43353\n",
      "[300]\tvalid_0's rmse: 1.41691\n",
      "[350]\tvalid_0's rmse: 1.40449\n"
     ]
    }
   ],
   "source": [
    "#Split data\n",
    "dev_X, val_X, dev_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "# Training LGB\n",
    "#pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\n",
    "print(\"LightGBM Training Completed...\")\n",
    "\n",
    "print(\"Features Importance...\")\n",
    "#gain = model.feature_importance('gain')\n",
    "#featureimp = pd.DataFrame({'feature':model.feature_name(), \n",
    "#                   'split':model.feature_importance('split'), \n",
    "#                   'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n",
    "#print(featureimp[:15])\n",
    "\n",
    "# Training XGB\n",
    "pred_test_xgb, model_xgb = run_xgb(dev_X, dev_y, val_X, val_y, X_test)\n",
    "print(\"XGB Training Completed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create file with submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID        target\n",
      "0  000137c73  3.676336e+06\n",
      "1  00021489f  2.111295e+06\n",
      "2  0004d7953  1.590406e+06\n",
      "3  00056a333  3.124707e+06\n",
      "4  00056d8eb  2.844350e+06\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv('../raw_files/sample_submission.csv')\n",
    "\n",
    "sub_lgb = pd.DataFrame()\n",
    "sub_lgb[\"target\"] = lg_an\n",
    "\n",
    "sub_xgb = pd.DataFrame()\n",
    "sub_xgb[\"target\"] = pred_test_xgb\n",
    "\n",
    "sub_cb = pd.DataFrame()\n",
    "sub_xgb[\"target\"] = cb_an\n",
    "\n",
    "sub[\"target\"] = (sub_lgb[\"target\"] + sub_xgb[\"target\"] + sub_cb[\"target\"])/3\n",
    "\n",
    "print(sub.head())\n",
    "sub.to_csv('sub_lgb_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
