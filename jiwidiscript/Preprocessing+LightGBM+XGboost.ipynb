{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FastICA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gc\n",
    "#Plotting\n",
    "import seaborn as sns\n",
    "\n",
    "#Models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the files and get a brief overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../raw_files/train.csv')\n",
    "test_df = pd.read_csv('../raw_files/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up train and test X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop([\"ID\", \"target\"], axis=1)\n",
    "y_train = np.log1p(train_df[\"target\"].values)\n",
    "\n",
    "X_test = test_df.drop([\"ID\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for NaN values and removing constant features in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Total Train Features with NaN Values = \" + str(train_df.columns[train_df.isnull().sum() != 0].size))\n",
    "if (train_df.columns[train_df.isnull().sum() != 0].size):\n",
    "    print(\"Features with NaN => {}\".format(list(train_df.columns[train_df.isnull().sum() != 0])))\n",
    "    train_df[train_df.columns[train_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_count = []\n",
    "for col in X_train.columns[2:]:\n",
    "    zero_count.append([i[1] for i in list(X_train[col].value_counts().items()) if i[0] == 0][0])\n",
    "    \n",
    "print('{0} features of 4491 have zeroes in 99% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.99])))\n",
    "print('{0} features of 4491 have zeroes in 98% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.98])))\n",
    "print('{0} features of 4491 have zeroes in 97% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.97])))\n",
    "print('{0} features of 4491 have zeroes in 96% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.96])))\n",
    "print('{0} features of 4491 have zeroes in 95% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.95])))\n",
    "\n",
    "cols_to_drop = [col for col in X_train.columns[2:] if [i[1] for i in list(X_train[col].value_counts().items()) if i[0] == 0][0] >= 4459 * 0.98]\n",
    "\n",
    "X_train.drop(cols_to_drop, axis=1, inplace=True)\n",
    "X_test.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colsToRemove = []\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].std() == 0: \n",
    "        colsToRemove.append(col)\n",
    "        \n",
    "# remove constant columns in the training set\n",
    "train_df.drop(colsToRemove, axis=1, inplace=True)\n",
    "\n",
    "# remove constant columns in the test set\n",
    "test_df.drop(colsToRemove, axis=1, inplace=True) \n",
    "\n",
    "print(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\n",
    "print(colsToRemove)\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToRemove = []\n",
    "colsScaned = []\n",
    "dupList = {}\n",
    "\n",
    "columns = X_train.columns\n",
    "\n",
    "for i in range(len(columns)-1):\n",
    "    v = X_train[columns[i]].values\n",
    "    dupCols = []\n",
    "    for j in range(i+1,len(columns)):\n",
    "        if np.array_equal(v, X_train[columns[j]].values):\n",
    "            colsToRemove.append(columns[j])\n",
    "            if columns[j] not in colsScaned:\n",
    "                dupCols.append(columns[j]) \n",
    "                colsScaned.append(columns[j])\n",
    "                dupList[columns[i]] = dupCols\n",
    "                \n",
    "# remove duplicate columns in the training set\n",
    "X_train.drop(colsToRemove, axis=1, inplace=True) \n",
    "\n",
    "# remove duplicate columns in the testing set\n",
    "X_test.drop(colsToRemove, axis=1, inplace=True)\n",
    "\n",
    "print(\"Removed `{}` Duplicate Columns\\n\".format(len(dupList)))\n",
    "print(dupList)\n",
    "\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Sparse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_sparse(train, test):\n",
    "    flist = [x for x in train.columns if not x in ['ID','target']]\n",
    "    for f in flist:\n",
    "        if len(np.unique(train[f]))<2:\n",
    "            train.drop(f, axis=1, inplace=True)\n",
    "            test.drop(f, axis=1, inplace=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drop_sparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c157a791d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTrain shape: {}\\nTest shape: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drop_sparse' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test = drop_sparse(X_train, X_test)\n",
    "\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumzeros and Sumvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_SumZeros(train, test, features):\n",
    "    flist = [x for x in train.columns if not x in ['ID','target']]\n",
    "    if 'SumZeros' in features:\n",
    "        train.insert(1, 'SumZeros', (train[flist] == 0).astype(int).sum(axis=1))\n",
    "        test.insert(1, 'SumZeros', (test[flist] == 0).astype(int).sum(axis=1))\n",
    "    flist = [x for x in train.columns if not x in ['ID','target']]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test = add_SumZeros(X_train, X_test, ['SumZeros'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_SumValues(train, test, features):\n",
    "    flist = [x for x in train.columns if not x in ['ID','target']]\n",
    "    if 'SumValues' in features:\n",
    "        train.insert(1, 'SumValues', (train[flist] != 0).astype(int).sum(axis=1))\n",
    "        test.insert(1, 'SumValues', (test[flist] != 0).astype(int).sum(axis=1))\n",
    "    flist = [x for x in train.columns if not x in ['ID','target']]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test = add_SumValues(X_train, X_test, ['SumValues'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_OtherAgg(train, test, features):\n",
    "    flist = [x for x in train.columns if not x in ['ID','target','SumZeros','SumValues']]\n",
    "    if 'OtherAgg' in features:\n",
    "        train['Mean'] = train.mean(axis=1)\n",
    "        train['Median'] = train.median(axis=1)\n",
    "        train['Mode'] = train.mode(axis=1)\n",
    "        train['Max'] = train.max(axis=1)\n",
    "        train['Var'] = train.var(axis=1)\n",
    "        train['Std'] = train.std(axis=1)\n",
    "        \n",
    "        test['Mean'] = test.mean(axis=1)\n",
    "        test['Median'] = test.median(axis=1)\n",
    "        test['Mode'] = test.mode(axis=1)\n",
    "        test['Max'] = test.max(axis=1)\n",
    "        test['Var'] = test.var(axis=1)\n",
    "        test['Std'] = test.std(axis=1)\n",
    "    flist = [x for x in train.columns if not x in ['ID','target','SumZeros','SumValues']]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kmeans_cluster_2', 'kmeans_cluster_3', 'kmeans_cluster_4', 'kmeans_cluster_5', 'kmeans_cluster_6', 'kmeans_cluster_7', 'kmeans_cluster_8', 'kmeans_cluster_9', 'kmeans_cluster_10']\n"
     ]
    }
   ],
   "source": [
    "def kmeans(X_Tr,Xte):\n",
    "    flist = [x for x in X_Tr.columns if not x in ['ID','target']]\n",
    "    flist_kmeans = []\n",
    "    for ncl in range(2,11):\n",
    "        cls = KMeans(n_clusters=ncl)\n",
    "        cls.fit_predict(X_train[flist].values)\n",
    "        X_Tr['kmeans_cluster_'+str(ncl)] = cls.predict(X_Tr[flist].values)\n",
    "        Xte['kmeans_cluster_'+str(ncl)] = cls.predict(Xte[flist].values)\n",
    "        flist_kmeans.append('kmeans_cluster_'+str(ncl))\n",
    "    print(flist_kmeans)\n",
    "    \n",
    "    return X_Tr,Xte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PCA_1', 'PCA_2', 'PCA_3', 'PCA_4', 'PCA_5', 'PCA_6', 'PCA_7', 'PCA_8', 'PCA_9', 'PCA_10', 'PCA_11', 'PCA_12', 'PCA_13', 'PCA_14', 'PCA_15', 'PCA_16', 'PCA_17', 'PCA_18', 'PCA_19', 'PCA_20']\n"
     ]
    }
   ],
   "source": [
    "def pca(X_Tr,Xte):\n",
    "    flist = [x for x in X_Tr.columns if not x in ['ID','target']]\n",
    "    n_components = 20\n",
    "    flist_pca = []\n",
    "    pca = PCA(n_components=n_components)\n",
    "    x_train_projected = pca.fit_transform(normalize(X_Tr[flist], axis=0))\n",
    "    x_test_projected = pca.transform(normalize(X_test[flist], axis=0))\n",
    "    for npca in range(0, n_components):\n",
    "        X_Tr.insert(1, 'PCA_'+str(npca+1), x_train_projected[:, npca])\n",
    "        Xte.insert(1, 'PCA_'+str(npca+1), x_test_projected[:, npca])\n",
    "        flist_pca.append('PCA_'+str(npca+1))\n",
    "    print(flist_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (4459, 2123)\n",
      "Test shape: (49342, 2123)\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start decomposition process...\n",
      "PCA\n",
      "tSVD\n",
      "ICA\n",
      "GRP\n",
      "SRP\n",
      "Append decomposition components to datasets...\n",
      "\n",
      "Train shape: (4459, 2223)\n",
      "Test shape: (49342, 2223)\n"
     ]
    }
   ],
   "source": [
    "PERC_TRESHOLD = 0.98   ### Percentage of zeros in each feature ###\n",
    "N_COMP = 20            ### Number of decomposition components ###\n",
    "\n",
    "print(\"\\nStart decomposition process...\")\n",
    "print(\"PCA\")\n",
    "pca = PCA(n_components=N_COMP, random_state=17)\n",
    "pca_results_train = pca.fit_transform(X_train)\n",
    "pca_results_test = pca.transform(X_test)\n",
    "\n",
    "print(\"tSVD\")\n",
    "tsvd = TruncatedSVD(n_components=N_COMP, random_state=17)\n",
    "tsvd_results_train = tsvd.fit_transform(X_train)\n",
    "tsvd_results_test = tsvd.transform(X_test)\n",
    "\n",
    "print(\"ICA\")\n",
    "ica = FastICA(n_components=N_COMP, random_state=17)\n",
    "ica_results_train = ica.fit_transform(X_train)\n",
    "ica_results_test = ica.transform(X_test)\n",
    "\n",
    "print(\"GRP\")\n",
    "grp = GaussianRandomProjection(n_components=N_COMP, eps=0.1, random_state=17)\n",
    "grp_results_train = grp.fit_transform(X_train)\n",
    "grp_results_test = grp.transform(X_test)\n",
    "\n",
    "print(\"SRP\")\n",
    "srp = SparseRandomProjection(n_components=N_COMP, dense_output=True, random_state=17)\n",
    "srp_results_train = srp.fit_transform(X_train)\n",
    "srp_results_test = srp.transform(X_test)\n",
    "\n",
    "print(\"Append decomposition components to datasets...\")\n",
    "for i in range(1, N_COMP + 1):\n",
    "    X_train['pca_' + str(i)] = pca_results_train[:, i - 1]\n",
    "    X_test['pca_' + str(i)] = pca_results_test[:, i - 1]\n",
    "\n",
    "    X_train['ica_' + str(i)] = ica_results_train[:, i - 1]\n",
    "    X_test['ica_' + str(i)] = ica_results_test[:, i - 1]\n",
    "\n",
    "    X_train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    X_test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    X_train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    X_test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    X_train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    X_test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"num_leaves\" : 40,\n",
    "        \"learning_rate\" : 0.005,\n",
    "        \"bagging_fraction\" : 0.7,\n",
    "        \"feature_fraction\" : 0.5,\n",
    "        \"bagging_frequency\" : 5,\n",
    "        \"bagging_seed\" : 42,\n",
    "        \"verbosity\" : -1,\n",
    "        \"random_seed\": 42\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgtrain, 5000, \n",
    "                      valid_sets=[lgval], \n",
    "                      early_stopping_rounds=100, \n",
    "                      verbose_eval=50, \n",
    "                      evals_result=evals_result)\n",
    "    \n",
    "    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n",
    "    return pred_test_y, model, evals_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {'objective': 'reg:linear', \n",
    "          'eval_metric': 'rmse',\n",
    "          'eta': 0.005,\n",
    "          'max_depth': 15, \n",
    "          'subsample': 0.7, \n",
    "          'colsample_bytree': 0.5,\n",
    "          'alpha':0,\n",
    "          'random_state':42, \n",
    "          'silent': True}\n",
    "    \n",
    "    tr_data = xgb.DMatrix(train_X, train_y)\n",
    "    va_data = xgb.DMatrix(val_X, val_y)\n",
    "    \n",
    "    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n",
    "    \n",
    "    model_xgb = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds = 30, verbose_eval=100)\n",
    "    \n",
    "    dtest = xgb.DMatrix(test_X)\n",
    "    xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n",
    "    \n",
    "    return xgb_pred_y, model_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmse: 1.60281\n",
      "[100]\tvalid_0's rmse: 1.53618\n",
      "[150]\tvalid_0's rmse: 1.48962\n",
      "[200]\tvalid_0's rmse: 1.4568\n",
      "[250]\tvalid_0's rmse: 1.43353\n",
      "[300]\tvalid_0's rmse: 1.41691\n",
      "[350]\tvalid_0's rmse: 1.40449\n"
     ]
    }
   ],
   "source": [
    "#Split data\n",
    "dev_X, val_X, dev_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "# Training LGB\n",
    "pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\n",
    "print(\"LightGBM Training Completed...\")\n",
    "\n",
    "print(\"Features Importance...\")\n",
    "gain = model.feature_importance('gain')\n",
    "featureimp = pd.DataFrame({'feature':model.feature_name(), \n",
    "                   'split':model.feature_importance('split'), \n",
    "                   'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n",
    "print(featureimp[:15])\n",
    "\n",
    "# Training XGB\n",
    "pred_test_xgb, model_xgb = run_xgb(dev_X, dev_y, val_X, val_y, X_test)\n",
    "print(\"XGB Training Completed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create file with submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID        target\n",
      "0  000137c73  3.676336e+06\n",
      "1  00021489f  2.111295e+06\n",
      "2  0004d7953  1.590406e+06\n",
      "3  00056a333  3.124707e+06\n",
      "4  00056d8eb  2.844350e+06\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv('../raw_files/sample_submission.csv')\n",
    "\n",
    "sub_lgb = pd.DataFrame()\n",
    "sub_lgb[\"target\"] = pred_test\n",
    "\n",
    "sub_xgb = pd.DataFrame()\n",
    "sub_xgb[\"target\"] = pred_test_xgb\n",
    "\n",
    "sub[\"target\"] = (sub_lgb[\"target\"] + sub_xgb[\"target\"])/2\n",
    "\n",
    "print(sub.head())\n",
    "sub.to_csv('sub_lgb_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
